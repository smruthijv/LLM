{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LIAR Train Data:\n",
      "Index(['id', 'label', 'text', 'subjects', 'speaker', 'job_title', 'state',\n",
      "       'party_affiliation', 'barely_true_count', 'false_count',\n",
      "       'half_true_count', 'mostly_true_count', 'pants_fire_count', 'context'],\n",
      "      dtype='object')\n",
      "\n",
      " LIAR Test Data:\n",
      "Index(['id', 'label', 'text', 'subjects', 'speaker', 'job_title', 'state',\n",
      "       'party_affiliation', 'barely_true_count', 'false_count',\n",
      "       'half_true_count', 'mostly_true_count', 'pants_fire_count', 'context'],\n",
      "      dtype='object')\n",
      "\n",
      " LIAR Valid Data:\n",
      "Index(['id', 'label', 'text', 'subjects', 'speaker', 'job_title', 'state',\n",
      "       'party_affiliation', 'barely_true_count', 'false_count',\n",
      "       'half_true_count', 'mostly_true_count', 'pants_fire_count', 'context'],\n",
      "      dtype='object')\n",
      "\n",
      " Gossip Fake Data:\n",
      "Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n",
      "\n",
      " Gossip Real Data:\n",
      "Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n",
      "\n",
      " Political Fake Data:\n",
      "Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n",
      "\n",
      " Political Real Data:\n",
      "Index(['id', 'news_url', 'title', 'tweet_ids'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Define dataset file paths\n",
    "datasets = {\n",
    "    \"LIAR Train\": \"train.tsv\",\n",
    "    \"LIAR Test\": \"test.tsv\",\n",
    "    \"LIAR Valid\": \"valid.tsv\",\n",
    "    \"Gossip Fake\": \"gossipcop_fake.csv\",\n",
    "    \"Gossip Real\": \"gossipcop_real.csv\",\n",
    "    \"Political Fake\": \"politifact_fake.csv\",\n",
    "    \"Political Real\": \"politifact_real.csv\"\n",
    "}\n",
    "\n",
    "# Define LIAR dataset column names\n",
    "liar_columns = [\n",
    "    \"id\", \"label\", \"text\", \"subjects\", \"speaker\", \"job_title\", \"state\",\n",
    "    \"party_affiliation\", \"barely_true_count\", \"false_count\", \"half_true_count\",\n",
    "    \"mostly_true_count\", \"pants_fire_count\", \"context\"\n",
    "]\n",
    "\n",
    "# Load datasets into a dictionary \n",
    "dataframes = {}\n",
    "for name, path in datasets.items():\n",
    "    sep = \"\\t\" if path.endswith(\".tsv\") else \",\"  # Detect separator\n",
    "    columns = liar_columns if \"LIAR\" in name else None  # Assign columns only for LIAR datasets\n",
    "    dataframes[name] = pd.read_csv(path, sep=sep, header=None if columns else \"infer\", names=columns)\n",
    "\n",
    "# Print first few rows of each dataset\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n {name} Data:\")\n",
    "    print(df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing: Standardizing Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  retain essential columns for the liar dataset\n",
    "liar_columns = [\"text\", \"label\", \"subjects\", \"context\", \"speaker\", \"party_affiliation\",\"barely_true_count\", \"false_count\", \"half_true_count\",\n",
    "    \"mostly_true_count\", \"pants_fire_count\", \"state\"]\n",
    "for key in [\"LIAR Train\", \"LIAR Test\", \"LIAR Valid\"]:\n",
    "    dataframes[key] = dataframes[key][liar_columns]\n",
    "\n",
    "#  manually add 'label' column before selecting other columns\n",
    "label_mapping = {\n",
    "    \"Gossip Fake\": \"fake\", \"Gossip Real\": \"real\",\n",
    "    \"Political Fake\": \"fake\", \"Political Real\": \"real\"\n",
    "}\n",
    "\n",
    "for key, label in label_mapping.items():\n",
    "    # first, create the label column\n",
    "    dataframes[key][\"label\"] = label  \n",
    "    \n",
    "    # check if 'title' and 'news_url' exist before renaming\n",
    "    expected_columns = [\"title\", \"news_url\", \"label\"]\n",
    "    available_columns = [col for col in expected_columns if col in dataframes[key].columns]\n",
    "    \n",
    "    if \"title\" in available_columns:\n",
    "        dataframes[key] = dataframes[key][available_columns].rename(columns={\"title\": \"text\"})\n",
    "    else:\n",
    "        print(f\" Warning: Column 'title' not found in {key}. Available columns: {dataframes[key].columns}\")\n",
    "\n",
    "# #  ensure all datasets have a consistent structure\n",
    "# for name, df in dataframes.items():\n",
    "#     print(f\"\\n {name} Data (After Filtering):\")\n",
    "#     print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LIAR Train Size of the datasets: (10240, 12)\n",
      "\n",
      " LIAR Test Size of the datasets: (1267, 12)\n",
      "\n",
      " LIAR Valid Size of the datasets: (1284, 12)\n",
      "\n",
      " Gossip Fake Size of the datasets: (5323, 3)\n",
      "\n",
      " Gossip Real Size of the datasets: (16817, 3)\n",
      "\n",
      " Political Fake Size of the datasets: (432, 3)\n",
      "\n",
      " Political Real Size of the datasets: (624, 3)\n"
     ]
    }
   ],
   "source": [
    "# check the size of the datasets\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n {name} Size of the datasets: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " LIAR Train missing value:\n",
      "text                    0\n",
      "label                   0\n",
      "subjects                2\n",
      "context               102\n",
      "speaker                 2\n",
      "party_affiliation       2\n",
      "barely_true_count       2\n",
      "false_count             2\n",
      "half_true_count         2\n",
      "mostly_true_count       2\n",
      "pants_fire_count        2\n",
      "state                2210\n",
      "dtype: int64\n",
      "\n",
      " LIAR Test missing value:\n",
      "text                   0\n",
      "label                  0\n",
      "subjects               0\n",
      "context               17\n",
      "speaker                0\n",
      "party_affiliation      0\n",
      "barely_true_count      0\n",
      "false_count            0\n",
      "half_true_count        0\n",
      "mostly_true_count      0\n",
      "pants_fire_count       0\n",
      "state                262\n",
      "dtype: int64\n",
      "\n",
      " LIAR Valid missing value:\n",
      "text                   0\n",
      "label                  0\n",
      "subjects               0\n",
      "context               12\n",
      "speaker                0\n",
      "party_affiliation      0\n",
      "barely_true_count      0\n",
      "false_count            0\n",
      "half_true_count        0\n",
      "mostly_true_count      0\n",
      "pants_fire_count       0\n",
      "state                279\n",
      "dtype: int64\n",
      "\n",
      " Gossip Fake missing value:\n",
      "text          0\n",
      "news_url    256\n",
      "label         0\n",
      "dtype: int64\n",
      "\n",
      " Gossip Real missing value:\n",
      "text         0\n",
      "news_url    13\n",
      "label        0\n",
      "dtype: int64\n",
      "\n",
      " Political Fake missing value:\n",
      "text        0\n",
      "news_url    4\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      " Political Real missing value:\n",
      "text         0\n",
      "news_url    57\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check missing value\n",
    "for name, df in dataframes.items():\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(f\"\\n {name} missing value:\")\n",
    "    print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text                  0.000000\n",
      "label                 0.000000\n",
      "subjects              0.019531\n",
      "context               0.996094\n",
      "speaker               0.019531\n",
      "party_affiliation     0.019531\n",
      "barely_true_count     0.019531\n",
      "false_count           0.019531\n",
      "half_true_count       0.019531\n",
      "mostly_true_count     0.019531\n",
      "pants_fire_count      0.019531\n",
      "state                21.582031\n",
      "dtype: float64\n",
      "text                  0.000000\n",
      "label                 0.000000\n",
      "subjects              0.000000\n",
      "context               1.341752\n",
      "speaker               0.000000\n",
      "party_affiliation     0.000000\n",
      "barely_true_count     0.000000\n",
      "false_count           0.000000\n",
      "half_true_count       0.000000\n",
      "mostly_true_count     0.000000\n",
      "pants_fire_count      0.000000\n",
      "state                20.678769\n",
      "dtype: float64\n",
      "text                  0.000000\n",
      "label                 0.000000\n",
      "subjects              0.000000\n",
      "context               0.934579\n",
      "speaker               0.000000\n",
      "party_affiliation     0.000000\n",
      "barely_true_count     0.000000\n",
      "false_count           0.000000\n",
      "half_true_count       0.000000\n",
      "mostly_true_count     0.000000\n",
      "pants_fire_count      0.000000\n",
      "state                21.728972\n",
      "dtype: float64\n",
      "text        0.000000\n",
      "news_url    4.809318\n",
      "label       0.000000\n",
      "dtype: float64\n",
      "text        0.000000\n",
      "news_url    0.077303\n",
      "label       0.000000\n",
      "dtype: float64\n",
      "text        0.000000\n",
      "news_url    0.925926\n",
      "label       0.000000\n",
      "dtype: float64\n",
      "text        0.000000\n",
      "news_url    9.134615\n",
      "label       0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# calculate the percentage of missing values for each dataset\n",
    "for name, df in dataframes.items():\n",
    "    missing_percentage = df.isnull().sum() / len(df) * 100  # Compute the missing value percentage    print(f\"\\n {name} Missing Value Percentage (%):\")\n",
    "    print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAR Train - Rows removed: 102\n",
      "LIAR Test - Rows removed: 17\n",
      "LIAR Valid - Rows removed: 12\n",
      "Gossip Fake - Rows removed: 256\n",
      "Gossip Real - Rows removed: 13\n",
      "Political Fake - Rows removed: 4\n",
      "Political Real - Rows removed: 57\n",
      "\n",
      "LIAR Train Missing Values After Fixing:\n",
      "text                 0\n",
      "label                0\n",
      "subjects             0\n",
      "context              0\n",
      "speaker              0\n",
      "party_affiliation    0\n",
      "barely_true_count    0\n",
      "false_count          0\n",
      "half_true_count      0\n",
      "mostly_true_count    0\n",
      "pants_fire_count     0\n",
      "state                0\n",
      "dtype: int64\n",
      "\n",
      "LIAR Test Missing Values After Fixing:\n",
      "text                 0\n",
      "label                0\n",
      "subjects             0\n",
      "context              0\n",
      "speaker              0\n",
      "party_affiliation    0\n",
      "barely_true_count    0\n",
      "false_count          0\n",
      "half_true_count      0\n",
      "mostly_true_count    0\n",
      "pants_fire_count     0\n",
      "state                0\n",
      "dtype: int64\n",
      "\n",
      "LIAR Valid Missing Values After Fixing:\n",
      "text                 0\n",
      "label                0\n",
      "subjects             0\n",
      "context              0\n",
      "speaker              0\n",
      "party_affiliation    0\n",
      "barely_true_count    0\n",
      "false_count          0\n",
      "half_true_count      0\n",
      "mostly_true_count    0\n",
      "pants_fire_count     0\n",
      "state                0\n",
      "dtype: int64\n",
      "\n",
      "Gossip Fake Missing Values After Fixing:\n",
      "text        0\n",
      "news_url    0\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      "Gossip Real Missing Values After Fixing:\n",
      "text        0\n",
      "news_url    0\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      "Political Fake Missing Values After Fixing:\n",
      "text        0\n",
      "news_url    0\n",
      "label       0\n",
      "dtype: int64\n",
      "\n",
      "Political Real Missing Values After Fixing:\n",
      "text        0\n",
      "news_url    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_5/gt0gf8wj36s11sdv_hybc58h0000gn/T/ipykernel_49379/111964742.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframes[key][\"state\"].fillna(\"Unknown\", inplace=True)\n",
      "/var/folders/_5/gt0gf8wj36s11sdv_hybc58h0000gn/T/ipykernel_49379/111964742.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframes[key][\"state\"].fillna(\"Unknown\", inplace=True)\n",
      "/var/folders/_5/gt0gf8wj36s11sdv_hybc58h0000gn/T/ipykernel_49379/111964742.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  dataframes[key][\"state\"].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values in specified columns for liar dataset\n",
    "for key in [\"LIAR Train\", \"LIAR Test\", \"LIAR Valid\"]:\n",
    "    initial_rows = dataframes[key].shape[0]\n",
    "    dataframes[key].dropna(subset=[\"subjects\", \"speaker\", \"party_affiliation\", \"context\"], inplace=True)\n",
    "    removed_rows = initial_rows - dataframes[key].shape[0]\n",
    "    print(f\"{key} - Rows removed: {removed_rows}\")\n",
    "    dataframes[key][\"state\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Remove rows with missing values in news_url for gossipcop and political dataset\n",
    "for key in [\"Gossip Fake\", \"Gossip Real\", \"Political Fake\", \"Political Real\"]:\n",
    "    initial_rows = dataframes[key].shape[0]\n",
    "    dataframes[key].dropna(subset=[\"news_url\"], inplace=True)\n",
    "    removed_rows = initial_rows - dataframes[key].shape[0]\n",
    "    print(f\"{key} - Rows removed: {removed_rows}\")\n",
    "\n",
    "# Make sure missing values are fixed\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n{name} Missing Values After Fixing:\")\n",
    "    print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove punctuation and stopwords\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    stop_words = set(stopwords.words(\"english\"))  # Load the stopword list\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "    words = text.split()  # Split the sentence into words\n",
    "    words = [word for word in words if word.lower() not in stop_words]  # Filter out stopwords\n",
    "    return \" \".join(words)  # Reassemble the cleaned words into a sentence\n",
    "\n",
    "# Apply clean_text() to all datasets\n",
    "for key in dataframes.keys():\n",
    "    dataframes[key][\"text\"] = dataframes[key][\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(api_key=\"YOUR API KEY\")  # Securely store API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing train/test/validation datasets and standardizing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIAR dataset\n",
    "liar_train = dataframes[\"LIAR Train\"]\n",
    "liar_test = dataframes[\"LIAR Test\"]\n",
    "liar_val = dataframes[\"LIAR Valid\"]\n",
    "# FakeNewsNet data\n",
    "gossipcop_fake = dataframes[\"Gossip Fake\"]\n",
    "gossipcop_real = dataframes[\"Gossip Real\"]\n",
    "politifact_fake = dataframes[\"Political Fake\"]\n",
    "politifact_real = dataframes[\"Political Real\"]\n",
    "\n",
    "label_mapping_binary = {\"fake\": 0, \"real\": 1}\n",
    "fake_news_df = pd.concat([gossipcop_fake, gossipcop_real, politifact_fake, politifact_real])\n",
    "fake_news_df['label'] = fake_news_df['label'].map(label_mapping_binary)\n",
    "\n",
    "# Standardize labels\n",
    "label_mapping_multi = {\"pants-fire\": 0, \"false\": 1, \"barely-true\": 2, \"half-true\": 3, \"mostly-true\": 4, \"true\": 5}\n",
    "liar_train['label'] = liar_train['label'].map(label_mapping_multi)\n",
    "liar_test['label'] = liar_test['label'].map(label_mapping_multi)\n",
    "liar_val['label'] = liar_val['label'].map(label_mapping_multi)\n",
    "\n",
    "\n",
    "# Train-test-validation split for Fake News Net dataset\n",
    "fake_news_train_df, temp_data = train_test_split(\n",
    "        fake_news_df,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=fake_news_df['label']\n",
    "    )\n",
    "\n",
    "fake_news_val_data, fake_news_test_data = train_test_split(\n",
    "        temp_data,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        stratify=temp_data['label']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for Fine Tuning and Token Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(data):\n",
    "    total_tokens = 0\n",
    "    for entry in data:\n",
    "        for key, value in entry.items():\n",
    "            if isinstance(value, str):\n",
    "                tokens = encoding.encode(value)  \n",
    "                total_tokens += len(tokens)\n",
    "            else:\n",
    "                tokens = encoding.encode(str(value))\n",
    "                total_tokens += len(tokens)\n",
    "    return total_tokens\n",
    "\n",
    "# Function to calculate the cost based on the number of tokens for GPT 3.5 ( Assumed cost)\n",
    "def calculate_cost(num_tokens, model=\"gpt-3.5-turbo\"):\n",
    "    cost_per_1000_tokens = 0.03  \n",
    "    cost = (num_tokens / 1000) * cost_per_1000_tokens\n",
    "    return cost\n",
    "\n",
    "# Function to prepare data for fine-tuning\n",
    "def prepare_fine_tune_data(df, dataset_type):    \n",
    "    fine_tune_data = []\n",
    "    if dataset_type == \"LIAR\":\n",
    "        label_map = {0: \"pants-fire\", 1: \"false\", 2: \"barely-true\", 3: \"half-true\", 4: \"mostly-true\", 5: \"true\"}\n",
    "    else:  \n",
    "        label_map = {0: \"fake\", 1: \"real\"}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        label = label_map.get(row['label'], \"false\" if dataset_type == \"LIAR\" else \"fake\")\n",
    "        system_message = \"You are a fact-checking assistant that labels news with a single word.\"\n",
    "        if dataset_type == \"LIAR\":\n",
    "            user_message = f\"\"\"Classify as pants-fire/false/barely-true/half-true/mostly-true/true:\n",
    "            Statement: {row['text']}\n",
    "            Speaker: {row.get('speaker', 'Unknown')} ({row.get('party_affiliation', 'Unknown')})\n",
    "            History: PF:{row.get('pants_fire_count', 0)}, F:{row.get('false_count', 0)}, BT:{row.get('barely_true_count', 0)}, HT:{row.get('half_true_count', 0)}, MT:{row.get('mostly_true_count', 0)}\"\"\"\n",
    "        else:  \n",
    "            \n",
    "            user_message = f\"\"\"Classify as real or fake:\n",
    "        News: {row['text']}\n",
    "        URL: {row.get('news_url', 'N/A')}\"\"\"\n",
    "        assistant_message = label\n",
    "        fine_tune_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_message}\n",
    "            ]\n",
    "        })\n",
    "    return fine_tune_data\n",
    "\n",
    "\n",
    "# Function to fine tune GPT using a training and validation dataset \n",
    "def fine_tune_gpt(training_data, validation_data, dataset_type, base_model=\"gpt-3.5-turbo\", epochs=4):\n",
    "    \"\"\"\n",
    "    Fine-tune with full dataset while ensuring balanced class distribution via oversampling.\n",
    "    Utilizes a separate validation dataset provided by the user.\n",
    "    \"\"\"\n",
    "    if training_data.empty:\n",
    "        raise ValueError(f\"Empty training data provided for {dataset_type}\")\n",
    "    if validation_data.empty:\n",
    "        raise ValueError(f\"Empty validation data provided for {dataset_type}\")\n",
    "\n",
    "    class_counts = training_data['label'].value_counts()\n",
    "    max_samples_per_class = class_counts.max()\n",
    "    \n",
    "    balanced_data = []\n",
    "    for class_value, count in class_counts.items():\n",
    "        class_data = training_data[training_data['label'] == class_value]\n",
    "        oversampled_data = class_data.sample(\n",
    "            n=max_samples_per_class, \n",
    "            replace=True, \n",
    "            random_state=42\n",
    "        )\n",
    "        balanced_data.append(oversampled_data)\n",
    "    \n",
    "    \n",
    "    training_data = pd.concat(balanced_data).sample(frac=1, random_state=42)\n",
    "    \n",
    "    fine_tune_data = prepare_fine_tune_data(training_data, dataset_type)\n",
    "    validation_data = prepare_fine_tune_data(validation_data, dataset_type)\n",
    "    \n",
    "    # Saving training data into jsonl file\n",
    "    train_file_name = f\"{dataset_type.lower()}_train_fine_tune_data.jsonl\"\n",
    "    with open(train_file_name, \"w\") as f:\n",
    "        for entry in fine_tune_data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    # Saving validation data into jsonl file\n",
    "    val_file_name = f\"{dataset_type.lower()}_val_fine_tune_data.jsonl\"\n",
    "    with open(val_file_name, \"w\") as f:\n",
    "        for entry in validation_data:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    \n",
    "    # Count tokens for cost calculation\n",
    "    train_token_count = count_tokens(fine_tune_data)\n",
    "    val_token_count = count_tokens(validation_data)\n",
    "    total_token_count = train_token_count + val_token_count\n",
    "    \n",
    "    # Calculating cost\n",
    "    total_cost = calculate_cost(total_token_count)\n",
    "    print(f\"Total tokens used: {total_token_count}\")\n",
    "    print(f\"Estimated cost for fine-tuning: ${total_cost:.2f}\")\n",
    "    \n",
    "    # Upload training and validation data\n",
    "    try:\n",
    "        with open(train_file_name, \"rb\") as f:\n",
    "            train_file_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "        train_file_id = train_file_response.id\n",
    "        \n",
    "        with open(val_file_name, \"rb\") as f:\n",
    "            val_file_response = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "        val_file_id = val_file_response.id\n",
    "        \n",
    "        wait_counter = 0\n",
    "        while wait_counter < 60:\n",
    "            train_status = client.files.retrieve(train_file_id)\n",
    "            val_status = client.files.retrieve(val_file_id)\n",
    "            if train_status.status == \"processed\" and val_status.status == \"processed\":\n",
    "                break\n",
    "            time.sleep(5)\n",
    "            wait_counter += 1\n",
    "        \n",
    "        suffix = f\"{dataset_type.lower()}_balanced_e{epochs}\"\n",
    "        fine_tune_response = client.fine_tuning.jobs.create(\n",
    "            training_file=train_file_id,\n",
    "            validation_file=val_file_id,\n",
    "            model=base_model,\n",
    "            hyperparameters={\n",
    "                \"n_epochs\": epochs,\n",
    "                \"batch_size\": 32,\n",
    "                \"learning_rate_multiplier\": 2e-5\n",
    "            },\n",
    "            suffix=suffix\n",
    "        )\n",
    "        \n",
    "        print(f\"Fine-tuning job created for {dataset_type}: {fine_tune_response.id}\")\n",
    "        return fine_tune_response.id\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during fine-tuning: {e}\")\n",
    "        if hasattr(e, 'response') and hasattr(e.response, 'json'):\n",
    "            print(f\"Error details: {e.response.json()}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions for Querying GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query GPT with retries \n",
    "def query_gpt_batch(prompts, model=\"gpt-3.5-turbo\", temperature=0.3, max_retries=3):\n",
    "    start_time = time.time()\n",
    "    responses = []\n",
    "    errors = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        retry_count = 0\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a fact-checking assistant that labels news as accurately as possible. Respond with exactly one word from the allowed set of labels.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=50,\n",
    "                    presence_penalty=0.0,\n",
    "                    frequency_penalty=0.0\n",
    "                )\n",
    "                \n",
    "                prediction = response.choices[0].message.content.strip().lower()\n",
    "                \n",
    "                # Validate prediction against expected label sets\n",
    "                if \"pants-fire\" in prompt or \"half-true\" in prompt:  # LIAR dataset\n",
    "                    valid_labels = [\"pants-fire\", \"false\", \"barely-true\", \"half-true\", \"mostly-true\", \"true\"]\n",
    "                    if not any(label in prediction for label in valid_labels):\n",
    "                        # If invalid response, retry with more explicit instructions\n",
    "                        if retry_count < max_retries - 1:\n",
    "                            retry_count += 1\n",
    "                            continue\n",
    "                else:  \n",
    "                    valid_labels = [\"fake\", \"real\"]\n",
    "                    if not any(label in prediction for label in valid_labels):\n",
    "                        if retry_count < max_retries - 1:\n",
    "                            retry_count += 1\n",
    "                            continue\n",
    "                \n",
    "                responses.append(prediction)\n",
    "                break  \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in API call (attempt {retry_count+1}/{max_retries}): {e}\")\n",
    "                retry_count += 1\n",
    "                if retry_count >= max_retries:\n",
    "                    print(f\"All retries failed. Using fallback response.\")\n",
    "                    responses.append(\"uncertain\")\n",
    "                    errors += 1\n",
    "                time.sleep(min(2 ** retry_count, 10))\n",
    "        \n",
    "    processing_time = time.time() - start_time\n",
    "    if errors > 0:\n",
    "        print(f\"Encountered {errors} errors in batch processing\")\n",
    "    \n",
    "    return responses, processing_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get few shot examples (Atleast one example per class)\n",
    "def get_few_shot_examples(df, label_key='label', num_examples=3):\n",
    "    examples = []\n",
    "    \n",
    "    if label_key in df.columns:\n",
    "        unique_labels = df[label_key].unique()\n",
    "        \n",
    "        for label in unique_labels:\n",
    "            label_df = df[df[label_key] == label]\n",
    "            if not label_df.empty:\n",
    "                examples_per_class = max(1, min(1, num_examples // len(unique_labels)))\n",
    "                examples.append(label_df.sample(n=min(examples_per_class, len(label_df)), random_state=42))\n",
    "        \n",
    "    examples_df = pd.concat(examples)\n",
    "    \n",
    "    return examples_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for generating efficient prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. \"Classify the news article based on its content: act as a news authenticity auditor and categorize the text as either pants-fire, false, barely-true, half-true, mostly-true, or true.\"', '   ', '2. \"Use your expertise to categorize news articles into one of six classes: pants-fire, false, barely-true, half-true, mostly-true, or true, reflecting the degree of truthfulness in the text.\"', '   ', '3. \"Employ your skills as a news classifier to assess and label articles as pants-fire, false, barely-true, half-true, mostly-true, or true, indicating the accuracy level of each report.\"']\n"
     ]
    }
   ],
   "source": [
    "# Generate prompts\n",
    "examples = get_few_shot_examples(liar_test, num_examples=3)\n",
    "\n",
    "example_texts = \"\\n\".join([\n",
    "                f\"Statement: '{row['text']}'\\nSpeaker: {row['speaker']} ({row['party_affiliation']})\\n\"\n",
    "                f\"History: PF:{row['pants_fire_count']}, F:{row['false_count']}, BT:{row['barely_true_count']}, \"\n",
    "                f\"HT:{row['half_true_count']}, MT:{row['mostly_true_count']} → {['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true'][row['label']]}\"\n",
    "                for _, row in examples.iterrows()\n",
    "            ])\n",
    "\n",
    "# Function to generate efficient prompt templates dynamically using GPT\n",
    "def generate_efficient_prompt_styles(example_texts, dataset_type = \"LIAR\", model=\"gpt-4\"):\n",
    "    \"\"\"Generate research-based efficient prompt templates dynamically using GPT.\"\"\"\n",
    "\n",
    "    if dataset_type == \"LIAR\":\n",
    "        classification_type = \"multi-class (pants-fire, false, barely-true, half-true, mostly-true, true)\"\n",
    "    else:\n",
    "        classification_type = \"binary (real/fake)\"\n",
    "    \n",
    "    system_prompt = f\"\"\"\n",
    "    You are an expert in prompt engineering and fake news detection. \n",
    "    Your task is to generate three highly efficient prompt templates for classifying news articles based on the dataset type. \n",
    "    The classification type is {classification_type}. The prompts should be concise, informative, and optimized for token efficiency and include act as.\n",
    "\n",
    "    Here are some example news articles and their labels:\n",
    "    {examples[:3]}\n",
    "\n",
    "    Please generate three distinct prompt templates optimized for {classification_type}.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    )\n",
    "\n",
    "    prompts = response.choices[0].message.content.split(\"\\n\")\n",
    "    return prompts\n",
    "\n",
    "\n",
    "\n",
    "print(generate_efficient_prompt_styles(example_texts,\"LIAR\",\"ft:gpt-3.5-turbo-0125:university-edinburgh:liar-balanced-e4:BHXpZ9MA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for various prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get prompt templates \n",
    "def get_prompt_templates(dataset_type, df):\n",
    "    examples = get_few_shot_examples(df, num_examples=6)\n",
    "\n",
    "    if dataset_type == \"LIAR\":\n",
    "        if not examples.empty:\n",
    "            example_texts = \"\\n\".join([\n",
    "                f\"Statement: '{row['text']}'\\nSpeaker: {row['speaker']} ({row['party_affiliation']})\\n\"\n",
    "                f\"History: PF:{row['pants_fire_count']}, F:{row['false_count']}, BT:{row['barely_true_count']}, \"\n",
    "                f\"HT:{row['half_true_count']}, MT:{row['mostly_true_count']} → {['pants-fire', 'false', 'barely-true', 'half-true', 'mostly-true', 'true'][row['label']]}\"\n",
    "                for _, row in examples.iterrows()\n",
    "            ])\n",
    "            \n",
    "            # Commented prompts were tried but had lower prediction accuracy\n",
    "            return [\n",
    "                # f\"\"\"Classify the news article based on its content: act as a news authenticity auditor \n",
    "                # and categorize the text as either pants-fire, false, barely-true, half-true, mostly-true, or true.\n",
    "                # \\nStatement: \"{{text}}\"\n",
    "                # Speaker: {{speaker}} ({{party}})\n",
    "                # History: PF:{{pants_fire}}, F:{{false}}, BT:{{barely_true}}, HT:{{half_true}}, MT:{{mostly_true}}\n",
    "                # \\nExamples:\n",
    "                # {example_texts}\n",
    "                # \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                # f\"\"\"Use your expertise to categorize news articles into one of six classes: \n",
    "                # pants-fire, false, barely-true, half-true, mostly-true, or true, reflecting the degree of truthfulness in the text.\n",
    "                # \\nStatement: \"{{text}}\"\n",
    "                # Speaker: {{speaker}} ({{party}})\n",
    "                # History: PF:{{pants_fire}}, F:{{false}}, BT:{{barely_true}}, HT:{{half_true}}, MT:{{mostly_true}}\n",
    "                # \\nExamples:\n",
    "                # {example_texts}\n",
    "                # \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                # f\"\"\"Employ your skills as a news classifier to assess and label articles as \n",
    "                # pants-fire, false, barely-true, half-true, mostly-true, or true, indicating the accuracy level of each report.\n",
    "                # \\nStatement: \"{{text}}\"\n",
    "                # Speaker: {{speaker}} ({{party}})\n",
    "                # History: PF:{{pants_fire}}, F:{{false}}, BT:{{barely_true}}, HT:{{half_true}}, MT:{{mostly_true}}\n",
    "                # \\nExamples:\n",
    "                # {example_texts}\n",
    "                # \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                f\"\"\"You are a fact-checking assistant that labels news with a single word.\n",
    "                \\nClassify as pants-fire/false/barely-true/half-true/mostly-true/true:\n",
    "                Statement: \"{{text}}\"\n",
    "                Speaker: {{speaker}} ({{party}})\n",
    "                History: PF:{{pants_fire}}, F:{{false}}, BT:{{barely_true}}, HT:{{half_true}}, MT:{{mostly_true}}\n",
    "                \\nConsider the claim's accuracy based on the speaker's history.\n",
    "                \\nExamples:\n",
    "                {example_texts}\n",
    "                \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                f\"\"\"You are a fact-checking assistant that labels news with a single word.\n",
    "                \\nAnalyze the accuracy of this statement based on the speaker's history:\n",
    "                \\nStatement: \"{{text}}\"\n",
    "                Speaker: {{speaker}} ({{party}})\n",
    "                History: PF:{{pants_fire}} (pants-fire), F:{{false}} (false), BT:{{barely_true}} (barely-true), HT:{{half_true}} (half-true), MT:{{mostly_true}} (mostly-true)\n",
    "                \\nExamples:\n",
    "                {example_texts}\n",
    "                \\n- Does the speaker have a history of making accurate claims?\n",
    "                - How does this statement compare to their past truthfulness?\n",
    "                \\nBased on this, classify as: pants-fire, false, barely-true, half-true, mostly-true, or true.\n",
    "                \\nClassification (one word only):\"\"\"\n",
    "                # ,\n",
    "\n",
    "\n",
    "                # f\"\"\"You are a fact-checking expert. Analyze this political statement by following these steps:\n",
    "                \n",
    "                # Statement: \"{{text}}\"\n",
    "                # Speaker: {{speaker}} ({{party}})\n",
    "                # Speaker's history: PF:{{pants_fire}}, F:{{false}}, BT:{{barely_true}}, HT:{{half_true}}, MT:{{mostly_true}}\n",
    "                \n",
    "                # Step 1: Identify the core claims in this statement.\n",
    "                # Step 2: Consider the speaker's history of truthfulness.\n",
    "                # Step 3: Analyze linguistic patterns for deception markers.\n",
    "                # Step 4: Classify as exactly one of: pants-fire, false, barely-true, half-true, mostly-true, true.\n",
    "                \n",
    "                # Examples:\n",
    "                # {example_texts}\n",
    "                \n",
    "                # Classification (one word only):\"\"\"\n",
    "            ]\n",
    "    else:\n",
    "        if not examples.empty:\n",
    "            example_texts = \"\\n\".join([\n",
    "                f\"News: '{row['text'][:50]}...'\\nURL: {row['news_url']} → {['fake', 'real'][row['label']]}\"\n",
    "                for _, row in examples.iterrows()\n",
    "            ])\n",
    "\n",
    "            return [\n",
    "                f\"\"\"You are a fact-checking assistant that labels news with a single word.\n",
    "                \\nClassify as real or fake:\n",
    "                News: \"{{text}}\"\n",
    "                URL: {{news_url}}\n",
    "                \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                f\"\"\"You are a fact-checking assistant that labels news with a single word.\n",
    "                \\nClassify as real or fake:\n",
    "                News: \"{{text}}\"\n",
    "                URL: {{news_url}}\n",
    "                \\nConsider factors such as credibility of the source, verifiability of claims, and potential biases.\n",
    "                \\nExamples:\n",
    "                {example_texts}\n",
    "                \\nClassification (one word only):\"\"\",\n",
    "                \n",
    "                f\"\"\"You are a fact-checking assistant that labels news with a single word.\n",
    "                \\nAnalyze this news for authenticity:\n",
    "                \\nNews: \"{{text}}\"\n",
    "                Source URL: {{news_url}}\n",
    "                \\n- Is the source known for reliable reporting?\n",
    "                - Can the claims in the article be independently verified?\n",
    "                - Does the article contain any signs of misinformation (sensationalism, lack of credible sources, etc.)?\n",
    "                \\nBased on this, classify as: real or fake.\n",
    "                \\nClassification (one word only):\"\"\"\n",
    "            ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pipeline and helper function to normalize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change predictions into integer format to match the processed true labels\n",
    "def normalize_prediction(prediction, dataset_type):\n",
    "    prediction = prediction.lower().strip()\n",
    "    if dataset_type == \"LIAR\":\n",
    "        if \"pants\" in prediction or \"fire\" in prediction:\n",
    "            return 0\n",
    "        elif \"false\" in prediction or \"fake\" in prediction and not any(x in prediction for x in [\"barely\", \"half\", \"mostly\"]):\n",
    "            return 1\n",
    "        elif \"barely\" in prediction:\n",
    "            return 2\n",
    "        elif \"half\" in prediction:\n",
    "            return 3\n",
    "        elif \"mostly\" in prediction:\n",
    "            return 4\n",
    "        elif \"true\" in prediction or \"real\" in prediction and not any(x in prediction for x in [\"barely\", \"half\", \"mostly\"]):\n",
    "            return 5\n",
    "        else:\n",
    "            return 1  \n",
    "    else:\n",
    "        if \"fake\" in prediction:\n",
    "            return 0\n",
    "        elif \"real\" in prediction:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0  \n",
    "        \n",
    "# Main function to run the pipeline of evaluating the model with test set\n",
    "def evaluate_model(df, dataset_type, batch_size=5, model=\"gpt-4\", temperature=0.1):\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=['label'])\n",
    "    try:\n",
    "        df['label'] = df['label'].astype(int)\n",
    "    except:\n",
    "        print(\"Warning: Could not convert labels to int. Attempting to use as-is.\")\n",
    "    \n",
    "    prompt_templates = get_prompt_templates(dataset_type, df)\n",
    "    print(\"Prompt Template\",prompt_templates)\n",
    "    results = []\n",
    "    \n",
    "    for template_idx, prompt_template in enumerate(prompt_templates):\n",
    "        print(f\"Evaluating with template {template_idx+1}...\")\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        total_tokens = 0\n",
    "        total_cost = 0\n",
    "        total_time = 0\n",
    "        batch_prompts = []\n",
    "        batch_rows = []  \n",
    "        \n",
    "        # Update the prompt template with individual row data\n",
    "        for i, row in enumerate(tqdm(df.iterrows(), total=len(df), desc=f\"Template {template_idx+1}\")):\n",
    "            idx, row_data = row  \n",
    "            \n",
    "            try:\n",
    "                if dataset_type == \"LIAR\":\n",
    "                    prompt = prompt_template.format(\n",
    "                        text=row_data['text'][:200],\n",
    "                        speaker=row_data.get('speaker', 'Unknown'),\n",
    "                        party=row_data.get('party_affiliation', 'Unknown'),\n",
    "                        pants_fire=row_data.get('pants_fire_count', 0),\n",
    "                        false=row_data.get('false_count', 0),\n",
    "                        barely_true=row_data.get('barely_true_count', 0),\n",
    "                        half_true=row_data.get('half_true_count', 0),\n",
    "                        mostly_true=row_data.get('mostly_true_count', 0)\n",
    "                    )\n",
    "                else:  \n",
    "                    prompt = prompt_template.format(\n",
    "                        text=row_data['text'][:200],\n",
    "                        news_url=row_data.get('news_url', 'N/A')\n",
    "                    )\n",
    "            except (KeyError, AttributeError) as e:\n",
    "                print(f\"Error formatting prompt: {e}\")\n",
    "                prompt = f\"Classify: {row_data.get('text', '')[:200]}\"\n",
    "            \n",
    "            batch_prompts.append(prompt)\n",
    "            batch_rows.append(row_data) \n",
    "\n",
    "            if len(batch_prompts) == batch_size or i == len(df) - 1:\n",
    "                if batch_prompts:\n",
    "                    try:\n",
    "                        # Query GPT with batch prompts\n",
    "                        predictions, processing_time = query_gpt_batch(batch_prompts, model=model, temperature=temperature)\n",
    "                    \n",
    "                        for j, (row_data, prediction) in enumerate(zip(batch_rows, predictions)):\n",
    "                            try:\n",
    "                                true_label = int(row_data['label'])\n",
    "                                normalized_pred = normalize_prediction(prediction, dataset_type)\n",
    "                            \n",
    "                                if dataset_type == \"LIAR\" and 0 <= true_label <= 5 and 0 <= normalized_pred <= 5:\n",
    "                                    y_true.append(true_label)\n",
    "                                    y_pred.append(normalized_pred)\n",
    "                                elif dataset_type == \"FAKENEWS\" and true_label in [0, 1] and normalized_pred in [0, 1]:\n",
    "                                    y_true.append(true_label)\n",
    "                                    y_pred.append(normalized_pred)\n",
    "                                else:\n",
    "                                    print(f\"Warning: Skipping example with invalid label or prediction: true={true_label}, pred={normalized_pred}\")\n",
    "                            except (ValueError, KeyError, TypeError) as e:\n",
    "                                print(f\"Error processing prediction: {e}\")\n",
    "                        \n",
    "                        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "                        total_time += processing_time\n",
    "                        prompt_tokens = sum(len(encoding.encode(p)) for p in batch_prompts)  \n",
    "                        completion_tokens = sum(len(encoding.encode(p)) for p in predictions)\n",
    "                        total_tokens += prompt_tokens + completion_tokens\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in batch processing: {e}\")\n",
    "            \n",
    "                batch_prompts = []\n",
    "                batch_rows = []\n",
    "\n",
    "        print(f\"Collected {len(y_true)} valid predictions for evaluation\")\n",
    "        if len(y_true) < 2:\n",
    "            print(\"WARNING: Not enough valid predictions collected, cannot calculate metrics\")\n",
    "            results.append({\n",
    "                'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0,\n",
    "                'avg_tokens': 0, 'cost': 0, 'avg_cost': 0, 'processing_time': 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            if dataset_type == \"FAKENEWS\":\n",
    "                if len(set(y_true)) == 1 or len(set(y_pred)) == 1:\n",
    "                    precision = accuracy\n",
    "                    recall = accuracy\n",
    "                    f1 = accuracy\n",
    "                else:\n",
    "                    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                        y_true, y_pred, average='binary', zero_division=0\n",
    "                    )\n",
    "            else:  \n",
    "                if len(set(y_true)) == 1 or len(set(y_pred)) == 1:\n",
    "                    precision = accuracy\n",
    "                    recall = accuracy\n",
    "                    f1 = accuracy\n",
    "                else:\n",
    "                    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                        y_true, y_pred, average='weighted', zero_division=0\n",
    "                    )\n",
    "            \n",
    "            if \"gpt-4\" in model:\n",
    "                cost_per_1k_tokens = 0.03\n",
    "            else:\n",
    "                cost_per_1k_tokens = 0.002\n",
    "            \n",
    "            total_cost = (total_tokens / 1000) * cost_per_1k_tokens\n",
    "            avg_tokens_per_article = total_tokens / len(df) if len(df) > 0 else 0\n",
    "            avg_cost_per_classification = total_cost / len(df) if len(df) > 0 else 0\n",
    "            avg_processing_time = total_time / len(df) if len(df) > 0 else 0\n",
    "            \n",
    "            result = {\n",
    "                'accuracy': accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1,\n",
    "                'avg_tokens': avg_tokens_per_article,\n",
    "                'cost': total_cost,\n",
    "                'avg_cost': avg_cost_per_classification,\n",
    "                'processing_time': avg_processing_time\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"\\nResults for {dataset_type} - Template {template_idx+1}:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "            print(f\"Average Tokens per Article: {avg_tokens_per_article:.2f}\")\n",
    "            print(f\"Total Cost: ${total_cost:.2f}\")\n",
    "            print(f\"Average Processing Time: {avg_processing_time:.2f}s\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating metrics: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            results.append({\n",
    "                'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0,\n",
    "                'avg_tokens': avg_tokens_per_article if 'avg_tokens_per_article' in locals() else 0,\n",
    "                'cost': total_cost if 'total_cost' in locals() else 0,\n",
    "                'avg_cost': avg_cost_per_classification if 'avg_cost_per_classification' in locals() else 0,\n",
    "                'processing_time': avg_processing_time if 'avg_processing_time' in locals() else 0\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tuning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuning LIAR classification model...\n",
      "Total tokens used: 1908725\n",
      "Estimated cost for fine-tuning: $57.26\n",
      "Fine-tuning job created for LIAR: ftjob-1I3bp8CLp8FoSjUkrP8g2MrZ\n",
      "\n",
      "Fine-tuning FakeNewsNet classification model...\n",
      "Total tokens used: 3007829\n",
      "Estimated cost for fine-tuning: $90.23\n",
      "Fine-tuning job created for FAKENEWS: ftjob-x0J8sR22sm9QCRkzfDL5gMfR\n",
      "\n",
      "Fine-tuning jobs submitted. Please wait for them to complete.\n",
      "LIAR fine-tune ID: ftjob-1I3bp8CLp8FoSjUkrP8g2MrZ\n",
      "FakeNewsNet fine-tune ID: ftjob-x0J8sR22sm9QCRkzfDL5gMfR\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "\n",
    "print(\"\\nFine-tuning LIAR classification model...\")\n",
    "liar_ft_id = fine_tune_gpt(liar_train, liar_val, \"LIAR\",\"gpt-3.5-turbo\", epochs=4)\n",
    "\n",
    "\n",
    "print(\"\\nFine-tuning FakeNewsNet classification model...\")\n",
    "fakenews_ft_id = fine_tune_gpt(fake_news_train_df, fake_news_val_data, \"FAKENEWS\", \"gpt-3.5-turbo\", epochs=4)\n",
    "\n",
    "\n",
    "print(\"\\nFine-tuning jobs submitted. Please wait for them to complete.\")\n",
    "print(f\"LIAR fine-tune ID: {liar_ft_id}\")\n",
    "print(f\"FakeNewsNet fine-tune ID: {fakenews_ft_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Fine-tuned GPT model for LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GPT-3.5-Turbo baseline...\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as pants-fire/false/barely-true/half-true/mostly-true/true:\\n                Statement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire}, F:{false}, BT:{barely_true}, HT:{half_true}, MT:{mostly_true}\\n                \\nConsider the claim\\'s accuracy based on the speaker\\'s history.\\n                \\nExamples:\\n                Statement: \\'pepper kicked jock tax imposing levy sports entertainment industry\\'\\nSpeaker: dave-yost (republican)\\nHistory: PF:0, F:0, BT:1, HT:0, MT:1 → true\\nStatement: \\'tell certainty capandtradewould devastating impact economy\\'\\nSpeaker: marco-rubio (republican)\\nHistory: PF:5, F:24, BT:33, HT:32, MT:35 → false\\nStatement: \\'austin school district teachers lowest paid urban texas district lowest paid surrounding school district\\'\\nSpeaker: gina-hinojosa (none)\\nHistory: PF:0, F:0, BT:0, HT:1, MT:1 → half-true\\nStatement: \\'78 81 house democrats members communist party called congressional progressive caucus\\'\\nSpeaker: allen-west (republican)\\nHistory: PF:4, F:9, BT:6, HT:3, MT:1 → pants-fire\\nStatement: \\'spend tax loopholes annually 11 trillion thats spend defense budget year medicare medicaid year\\'\\nSpeaker: jeanne-shaheen (democrat)\\nHistory: PF:0, F:0, BT:3, HT:4, MT:2 → barely-true\\nStatement: \\'says statistics indicate one eight children one 18 adults oregon suffers mental illness\\'\\nSpeaker: peter-courtney (democrat)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:1 → mostly-true\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze the accuracy of this statement based on the speaker\\'s history:\\n                \\nStatement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire} (pants-fire), F:{false} (false), BT:{barely_true} (barely-true), HT:{half_true} (half-true), MT:{mostly_true} (mostly-true)\\n                \\nExamples:\\n                Statement: \\'pepper kicked jock tax imposing levy sports entertainment industry\\'\\nSpeaker: dave-yost (republican)\\nHistory: PF:0, F:0, BT:1, HT:0, MT:1 → true\\nStatement: \\'tell certainty capandtradewould devastating impact economy\\'\\nSpeaker: marco-rubio (republican)\\nHistory: PF:5, F:24, BT:33, HT:32, MT:35 → false\\nStatement: \\'austin school district teachers lowest paid urban texas district lowest paid surrounding school district\\'\\nSpeaker: gina-hinojosa (none)\\nHistory: PF:0, F:0, BT:0, HT:1, MT:1 → half-true\\nStatement: \\'78 81 house democrats members communist party called congressional progressive caucus\\'\\nSpeaker: allen-west (republican)\\nHistory: PF:4, F:9, BT:6, HT:3, MT:1 → pants-fire\\nStatement: \\'spend tax loopholes annually 11 trillion thats spend defense budget year medicare medicaid year\\'\\nSpeaker: jeanne-shaheen (democrat)\\nHistory: PF:0, F:0, BT:3, HT:4, MT:2 → barely-true\\nStatement: \\'says statistics indicate one eight children one 18 adults oregon suffers mental illness\\'\\nSpeaker: peter-courtney (democrat)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:1 → mostly-true\\n                \\n- Does the speaker have a history of making accurate claims?\\n                - How does this statement compare to their past truthfulness?\\n                \\nBased on this, classify as: pants-fire, false, barely-true, half-true, mostly-true, or true.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 1250/1250 [16:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 1:\n",
      "Accuracy: 0.2416\n",
      "Precision: 0.3121\n",
      "Recall: 0.2416\n",
      "F1 Score: 0.2171\n",
      "Average Tokens per Article: 438.82\n",
      "Total Cost: $1.10\n",
      "Average Processing Time: 0.79s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 1250/1250 [16:50<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 2:\n",
      "Accuracy: 0.2824\n",
      "Precision: 0.3415\n",
      "Recall: 0.2824\n",
      "F1 Score: 0.2368\n",
      "Average Tokens per Article: 489.30\n",
      "Total Cost: $1.22\n",
      "Average Processing Time: 0.81s\n",
      "\n",
      "# Evaluating models with fine tuned model:\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as pants-fire/false/barely-true/half-true/mostly-true/true:\\n                Statement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire}, F:{false}, BT:{barely_true}, HT:{half_true}, MT:{mostly_true}\\n                \\nConsider the claim\\'s accuracy based on the speaker\\'s history.\\n                \\nExamples:\\n                Statement: \\'pepper kicked jock tax imposing levy sports entertainment industry\\'\\nSpeaker: dave-yost (republican)\\nHistory: PF:0, F:0, BT:1, HT:0, MT:1 → true\\nStatement: \\'tell certainty capandtradewould devastating impact economy\\'\\nSpeaker: marco-rubio (republican)\\nHistory: PF:5, F:24, BT:33, HT:32, MT:35 → false\\nStatement: \\'austin school district teachers lowest paid urban texas district lowest paid surrounding school district\\'\\nSpeaker: gina-hinojosa (none)\\nHistory: PF:0, F:0, BT:0, HT:1, MT:1 → half-true\\nStatement: \\'78 81 house democrats members communist party called congressional progressive caucus\\'\\nSpeaker: allen-west (republican)\\nHistory: PF:4, F:9, BT:6, HT:3, MT:1 → pants-fire\\nStatement: \\'spend tax loopholes annually 11 trillion thats spend defense budget year medicare medicaid year\\'\\nSpeaker: jeanne-shaheen (democrat)\\nHistory: PF:0, F:0, BT:3, HT:4, MT:2 → barely-true\\nStatement: \\'says statistics indicate one eight children one 18 adults oregon suffers mental illness\\'\\nSpeaker: peter-courtney (democrat)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:1 → mostly-true\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze the accuracy of this statement based on the speaker\\'s history:\\n                \\nStatement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire} (pants-fire), F:{false} (false), BT:{barely_true} (barely-true), HT:{half_true} (half-true), MT:{mostly_true} (mostly-true)\\n                \\nExamples:\\n                Statement: \\'pepper kicked jock tax imposing levy sports entertainment industry\\'\\nSpeaker: dave-yost (republican)\\nHistory: PF:0, F:0, BT:1, HT:0, MT:1 → true\\nStatement: \\'tell certainty capandtradewould devastating impact economy\\'\\nSpeaker: marco-rubio (republican)\\nHistory: PF:5, F:24, BT:33, HT:32, MT:35 → false\\nStatement: \\'austin school district teachers lowest paid urban texas district lowest paid surrounding school district\\'\\nSpeaker: gina-hinojosa (none)\\nHistory: PF:0, F:0, BT:0, HT:1, MT:1 → half-true\\nStatement: \\'78 81 house democrats members communist party called congressional progressive caucus\\'\\nSpeaker: allen-west (republican)\\nHistory: PF:4, F:9, BT:6, HT:3, MT:1 → pants-fire\\nStatement: \\'spend tax loopholes annually 11 trillion thats spend defense budget year medicare medicaid year\\'\\nSpeaker: jeanne-shaheen (democrat)\\nHistory: PF:0, F:0, BT:3, HT:4, MT:2 → barely-true\\nStatement: \\'says statistics indicate one eight children one 18 adults oregon suffers mental illness\\'\\nSpeaker: peter-courtney (democrat)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:1 → mostly-true\\n                \\n- Does the speaker have a history of making accurate claims?\\n                - How does this statement compare to their past truthfulness?\\n                \\nBased on this, classify as: pants-fire, false, barely-true, half-true, mostly-true, or true.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 1250/1250 [33:45<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 1:\n",
      "Accuracy: 0.3120\n",
      "Precision: 0.4748\n",
      "Recall: 0.3120\n",
      "F1 Score: 0.2787\n",
      "Average Tokens per Article: 437.96\n",
      "Total Cost: $1.09\n",
      "Average Processing Time: 1.62s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 1250/1250 [25:02<00:00,  1.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 2:\n",
      "Accuracy: 0.3728\n",
      "Precision: 0.4633\n",
      "Recall: 0.3728\n",
      "F1 Score: 0.3550\n",
      "Average Tokens per Article: 489.21\n",
      "Total Cost: $1.22\n",
      "Average Processing Time: 1.20s\n",
      "LIAR Template 1: F1 improved by 28.40% from 0.2171 to 0.2787\n",
      "LIAR Template 2: F1 improved by 49.93% from 0.2368 to 0.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned gpt model that was fine-tuned with Liar Data ( The model id is obtained using fine-tune ID generated above)\n",
    "liar_model_id = 'ft:gpt-3.5-turbo-0125:university-edinburgh:liar-balanced-e4:BHkIJsSZ'\n",
    "\n",
    "print(\"\\nEvaluating GPT-3.5-Turbo baseline...\")\n",
    "liar_baseline = evaluate_model(liar_test, \"LIAR\", batch_size=10, model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "print(\"\\n# Evaluating models with fine tuned model:\")\n",
    "liar_final_metrics = evaluate_model(liar_test, 'LIAR', batch_size=10, model=liar_model_id, temperature=0.3)\n",
    "\n",
    "\n",
    "for i, metrics in enumerate(liar_final_metrics):\n",
    "    baseline_f1 = liar_baseline[i]['f1']\n",
    "    improved_f1 = metrics['f1']\n",
    "    improvement = (improved_f1 - baseline_f1) / baseline_f1 * 100\n",
    "    print(f'LIAR Template {i+1}: F1 improved by {improvement:.2f}% from {baseline_f1:.4f} to {improved_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Fine-tuned GPT model for FakeNewsNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating GPT-3.5-Turbo baseline...\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nConsider factors such as credibility of the source, verifiability of claims, and potential biases.\\n                \\nExamples:\\n                News: \\'scarlett johansson colin jost american museum gala...\\'\\nURL: www.popsugar.com/celebrity/Scarlett-Johansson-Colin-Jost-American-Museum-Gala-2017-44319959 → fake\\nNews: \\'prince michael jackson talks carrying father micha...\\'\\nURL: https://www.longroom.com/discussion/493103/prince-michael-jackson-talks-carrying-on-father-michael-jacksons-philanthropic-legacy-at-2017-billboard-music-awards → real\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze this news for authenticity:\\n                \\nNews: \"{text}\"\\n                Source URL: {news_url}\\n                \\n- Is the source known for reliable reporting?\\n                - Can the claims in the article be independently verified?\\n                - Does the article contain any signs of misinformation (sensationalism, lack of credible sources, etc.)?\\n                \\nBased on this, classify as: real or fake.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 3430/3430 [40:44<00:00,  1.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 1:\n",
      "Accuracy: 0.8085\n",
      "Precision: 0.8338\n",
      "Recall: 0.9340\n",
      "F1 Score: 0.8810\n",
      "Average Tokens per Article: 82.08\n",
      "Total Cost: $0.56\n",
      "Average Processing Time: 0.71s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 3430/3430 [37:04<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 2:\n",
      "Accuracy: 0.8163\n",
      "Precision: 0.8360\n",
      "Recall: 0.9432\n",
      "F1 Score: 0.8864\n",
      "Average Tokens per Article: 224.07\n",
      "Total Cost: $1.54\n",
      "Average Processing Time: 0.65s\n",
      "Evaluating with template 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 3: 100%|██████████| 3430/3430 [51:25<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 3:\n",
      "Accuracy: 0.7787\n",
      "Precision: 0.8683\n",
      "Recall: 0.8353\n",
      "F1 Score: 0.8515\n",
      "Average Tokens per Article: 141.29\n",
      "Total Cost: $0.97\n",
      "Average Processing Time: 0.90s\n",
      "\n",
      "# Evaluating models with fine tuned model:\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nConsider factors such as credibility of the source, verifiability of claims, and potential biases.\\n                \\nExamples:\\n                News: \\'scarlett johansson colin jost american museum gala...\\'\\nURL: www.popsugar.com/celebrity/Scarlett-Johansson-Colin-Jost-American-Museum-Gala-2017-44319959 → fake\\nNews: \\'prince michael jackson talks carrying father micha...\\'\\nURL: https://www.longroom.com/discussion/493103/prince-michael-jackson-talks-carrying-on-father-michael-jacksons-philanthropic-legacy-at-2017-billboard-music-awards → real\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze this news for authenticity:\\n                \\nNews: \"{text}\"\\n                Source URL: {news_url}\\n                \\n- Is the source known for reliable reporting?\\n                - Can the claims in the article be independently verified?\\n                - Does the article contain any signs of misinformation (sensationalism, lack of credible sources, etc.)?\\n                \\nBased on this, classify as: real or fake.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1:  80%|███████▉  | 2730/3430 [50:32<11:41,  1.00s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 1/3): Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1:  86%|████████▌ | 2937/3430 [54:07<07:40,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 1/3): <!DOCTYPE html>\n",
      "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
      "<head>\n",
      "\n",
      "\n",
      "<title>api.openai.com | 520: Web server is returning an unknown error</title>\n",
      "<meta charset=\"UTF-8\" />\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
      "<meta name=\"robots\" content=\"noindex, nofollow\" />\n",
      "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
      "<link rel=\"stylesheet\" id=\"cf_styles-css\" href=\"/cdn-cgi/styles/main.css\" />\n",
      "\n",
      "\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"cf-wrapper\">\n",
      "    <div id=\"cf-error-details\" class=\"p-0\">\n",
      "        <header class=\"mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-8\">\n",
      "            <h1 class=\"inline-block sm:block sm:mb-2 font-light text-60 lg:text-4xl text-black-dark leading-tight mr-2\">\n",
      "              <span class=\"inline-block\">Web server is returning an unknown error</span>\n",
      "              <span class=\"code-label\">Error code 520</span>\n",
      "            </h1>\n",
      "            <div>\n",
      "               Visit <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_520&utm_campaign=api.openai.com\" target=\"_blank\" rel=\"noopener noreferrer\">cloudflare.com</a> for more information.\n",
      "            </div>\n",
      "            <div class=\"mt-3\">2025-04-02 17:14:54 UTC</div>\n",
      "        </header>\n",
      "        <div class=\"my-8 bg-gradient-gray\">\n",
      "            <div class=\"w-240 lg:w-full mx-auto\">\n",
      "                <div class=\"clearfix md:px-8\">\n",
      "                  \n",
      "<div id=\"cf-browser-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    \n",
      "    <span class=\"cf-icon-browser block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    \n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">You</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    \n",
      "    Browser\n",
      "    \n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
      "</div>\n",
      "\n",
      "<div id=\"cf-cloudflare-status\" class=\" relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_520&utm_campaign=api.openai.com\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
      "    <span class=\"cf-icon-cloud block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-ok w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    </a>\n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">Manchester</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    <a href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_520&utm_campaign=api.openai.com\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
      "    Cloudflare\n",
      "    </a>\n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-green-success\">Working</span>\n",
      "</div>\n",
      "\n",
      "<div id=\"cf-host-status\" class=\"cf-error-source relative w-1/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\">\n",
      "  <div class=\"relative mb-10 md:m-0\">\n",
      "    \n",
      "    <span class=\"cf-icon-server block md:hidden h-20 bg-center bg-no-repeat\"></span>\n",
      "    <span class=\"cf-icon-error w-12 h-12 absolute left-1/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"></span>\n",
      "    \n",
      "  </div>\n",
      "  <span class=\"md:block w-full truncate\">api.openai.com</span>\n",
      "  <h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\">\n",
      "    \n",
      "    Host\n",
      "    \n",
      "  </h3>\n",
      "  <span class=\"leading-1.3 text-2xl text-red-error\">Error</span>\n",
      "</div>\n",
      "\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "\n",
      "        <div class=\"w-240 lg:w-full mx-auto mb-8 lg:px-8\">\n",
      "            <div class=\"clearfix\">\n",
      "                <div class=\"w-1/2 md:w-full float-left pr-6 md:pb-10 md:pr-0 leading-relaxed\">\n",
      "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What happened?</h2>\n",
      "                    <p>There is an unknown connection issue between Cloudflare and the origin web server. As a result, the web page can not be displayed.</p>\n",
      "                </div>\n",
      "                <div class=\"w-1/2 md:w-full float-left leading-relaxed\">\n",
      "                    <h2 class=\"text-3xl font-normal leading-1.3 mb-4\">What can I do?</h2>\n",
      "                          <h3 class=\"text-15 font-semibold mb-2\">If you are a visitor of this website:</h3>\n",
      "      <p class=\"mb-6\">Please try again in a few minutes.</p>\n",
      "\n",
      "      <h3 class=\"text-15 font-semibold mb-2\">If you are the owner of this website:</h3>\n",
      "      <p><span>There is an issue between Cloudflare's cache and your origin web server. Cloudflare monitors for these errors and automatically investigates the cause. To help support the investigation, you can pull the corresponding error log from your web server and submit it our support team.  Please include the Ray ID (which is at the bottom of this error page).</span> <a rel=\"noopener noreferrer\" href=\"https://support.cloudflare.com/hc/en-us/articles/200171936-Error-520\">Additional troubleshooting resources</a>.</p>\n",
      "                </div>\n",
      "            </div>\n",
      "        </div>\n",
      "\n",
      "        <div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\">\n",
      "  <p class=\"text-13\">\n",
      "    <span class=\"cf-footer-item sm:block sm:mb-1\">Cloudflare Ray ID: <strong class=\"font-semibold\">92a1f717bb84b3c0</strong></span>\n",
      "    <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
      "    <span id=\"cf-footer-item-ip\" class=\"cf-footer-item hidden sm:block sm:mb-1\">\n",
      "      Your IP:\n",
      "      <button type=\"button\" id=\"cf-footer-ip-reveal\" class=\"cf-footer-ip-reveal-btn\">Click to reveal</button>\n",
      "      <span class=\"hidden\" id=\"cf-footer-ip\">192.41.125.254</span>\n",
      "      <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
      "    </span>\n",
      "    <span class=\"cf-footer-item sm:block sm:mb-1\"><span>Performance &amp; security by</span> <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com/5xx-error-landing?utm_source=errorcode_520&utm_campaign=api.openai.com\" id=\"brand_link\" target=\"_blank\">Cloudflare</a></span>\n",
      "    \n",
      "  </p>\n",
      "  <script>(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();</script>\n",
      "</div><!-- /.error-footer -->\n",
      "\n",
      "\n",
      "    </div>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 3430/3430 [1:03:07<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 1:\n",
      "Accuracy: 0.8009\n",
      "Precision: 0.8344\n",
      "Recall: 0.9205\n",
      "F1 Score: 0.8753\n",
      "Average Tokens per Article: 82.07\n",
      "Total Cost: $0.56\n",
      "Average Processing Time: 1.10s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2:  52%|█████▏    | 1779/3430 [30:07<25:53,  1.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 1/3): Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2:  55%|█████▌    | 1893/3430 [32:11<33:02,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 1/3): Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2:  57%|█████▋    | 1938/3430 [32:58<22:31,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 1/3): Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 3430/3430 [58:28<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 2:\n",
      "Accuracy: 0.8117\n",
      "Precision: 0.8395\n",
      "Recall: 0.9298\n",
      "F1 Score: 0.8823\n",
      "Average Tokens per Article: 224.07\n",
      "Total Cost: $1.54\n",
      "Average Processing Time: 1.02s\n",
      "Evaluating with template 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 3:   5%|▌         | 174/3430 [03:35<52:43,  1.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in API call (attempt 3/3): Error code: 500 - {'error': {'message': 'The server had an error while processing your request. Sorry about that!', 'type': 'server_error', 'param': None, 'code': None}}\n",
      "All retries failed. Using fallback response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 3:   5%|▌         | 177/3430 [03:53<2:14:29,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encountered 1 errors in batch processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 3: 100%|██████████| 3430/3430 [1:05:14<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 3:\n",
      "Accuracy: 0.7860\n",
      "Precision: 0.8633\n",
      "Recall: 0.8534\n",
      "F1 Score: 0.8583\n",
      "Average Tokens per Article: 141.22\n",
      "Total Cost: $0.97\n",
      "Average Processing Time: 1.14s\n",
      "FakeNewsNet Template 1: F1 improved by -0.65% from 0.8810 to 0.8753\n",
      "FakeNewsNet Template 2: F1 improved by -0.45% from 0.8864 to 0.8823\n",
      "FakeNewsNet Template 3: F1 improved by 0.80% from 0.8515 to 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the fine-tuned gpt model that was trained with FakeNewsNet( The model id is obtained using fine-tune ID generated above)\n",
    "fakenews_model_id = 'ft:gpt-3.5-turbo-0125:university-edinburgh:fakenews-balanced-e4:BHlR3dso'\n",
    "\n",
    "print(\"\\nEvaluating GPT-3.5-Turbo baseline...\")\n",
    "fakenews_baseline = evaluate_model(fake_news_test_data, \"FAKENEWS\", batch_size=10, model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "\n",
    "print(\"\\n# Evaluating models with fine tuned model:\")\n",
    "fakenews_final_metrics = evaluate_model(fake_news_test_data, 'FAKENEWS', batch_size=3, model=fakenews_model_id)\n",
    "\n",
    "for i, metrics in enumerate(fakenews_final_metrics):\n",
    "    baseline_f1 = fakenews_baseline[i]['f1']\n",
    "    improved_f1 = metrics['f1']\n",
    "    improvement = (improved_f1 - baseline_f1) / baseline_f1 * 100\n",
    "    print(f'FakeNewsNet Template {i+1}: F1 improved by {improvement:.2f}% from {baseline_f1:.4f} to {improved_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Fake News Labels to match LIAR labels\n",
    "def map_fake_news_to_liar(label):\n",
    "        return 0 if label == 0 else 5  \n",
    "\n",
    "# Map LIAR Labels to match FakeNewsNet labels\n",
    "def map_liar_to_binary(label):\n",
    "    return 0 if label <= 2 else 1\n",
    "\n",
    "\n",
    "liar_test['label'] = liar_test['label'].apply(map_liar_to_binary)\n",
    "fake_news_test_data['label'] = fake_news_test_data['label'].apply(map_fake_news_to_liar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_test['news_url'] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news_test_data['speaker'] = \"Unknown\"\n",
    "fake_news_test_data['party_affiliation'] = \"Unknown\"\n",
    "fake_news_test_data['pants_fire_count'] = 0\n",
    "fake_news_test_data['false_count'] = 0\n",
    "fake_news_test_data['barely_true_count'] = 0\n",
    "fake_news_test_data['half_true_count'] = 0\n",
    "fake_news_test_data['mostly_true_count'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Cross dataset evaluation:\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as pants-fire/false/barely-true/half-true/mostly-true/true:\\n                Statement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire}, F:{false}, BT:{barely_true}, HT:{half_true}, MT:{mostly_true}\\n                \\nConsider the claim\\'s accuracy based on the speaker\\'s history.\\n                \\nExamples:\\n                Statement: \\'scarlett johansson colin jost american museum gala 2017\\'\\nSpeaker: Unknown (Unknown)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:0 → pants-fire\\nStatement: \\'prince michael jackson talks carrying father michael jacksons philanthropic legacy 2017 billboard music awards\\'\\nSpeaker: Unknown (Unknown)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:0 → true\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze the accuracy of this statement based on the speaker\\'s history:\\n                \\nStatement: \"{text}\"\\n                Speaker: {speaker} ({party})\\n                History: PF:{pants_fire} (pants-fire), F:{false} (false), BT:{barely_true} (barely-true), HT:{half_true} (half-true), MT:{mostly_true} (mostly-true)\\n                \\nExamples:\\n                Statement: \\'scarlett johansson colin jost american museum gala 2017\\'\\nSpeaker: Unknown (Unknown)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:0 → pants-fire\\nStatement: \\'prince michael jackson talks carrying father michael jacksons philanthropic legacy 2017 billboard music awards\\'\\nSpeaker: Unknown (Unknown)\\nHistory: PF:0, F:0, BT:0, HT:0, MT:0 → true\\n                \\n- Does the speaker have a history of making accurate claims?\\n                - How does this statement compare to their past truthfulness?\\n                \\nBased on this, classify as: pants-fire, false, barely-true, half-true, mostly-true, or true.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 3430/3430 [51:39<00:00,  1.11it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 1:\n",
      "Accuracy: 0.7166\n",
      "Precision: 0.7546\n",
      "Recall: 0.7166\n",
      "F1 Score: 0.7155\n",
      "Average Tokens per Article: 214.72\n",
      "Total Cost: $1.47\n",
      "Average Processing Time: 0.90s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 3430/3430 [56:07<00:00,  1.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 3430 valid predictions for evaluation\n",
      "\n",
      "Results for LIAR - Template 2:\n",
      "Accuracy: 0.6329\n",
      "Precision: 0.7645\n",
      "Recall: 0.6329\n",
      "F1 Score: 0.6806\n",
      "Average Tokens per Article: 265.72\n",
      "Total Cost: $1.82\n",
      "Average Processing Time: 0.98s\n",
      "Prompt Template ['You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nClassify as real or fake:\\n                News: \"{text}\"\\n                URL: {news_url}\\n                \\nConsider factors such as credibility of the source, verifiability of claims, and potential biases.\\n                \\nExamples:\\n                News: \\'says rick scotts record jobs includes florida rank...\\'\\nURL: Unknown → real\\nNews: \\'know saddam hussein well killed terrorists...\\'\\nURL: Unknown → fake\\n                \\nClassification (one word only):', 'You are a fact-checking assistant that labels news with a single word.\\n                \\nAnalyze this news for authenticity:\\n                \\nNews: \"{text}\"\\n                Source URL: {news_url}\\n                \\n- Is the source known for reliable reporting?\\n                - Can the claims in the article be independently verified?\\n                - Does the article contain any signs of misinformation (sensationalism, lack of credible sources, etc.)?\\n                \\nBased on this, classify as: real or fake.\\n                \\nClassification (one word only):']\n",
      "Evaluating with template 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 1: 100%|██████████| 1250/1250 [18:03<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 1:\n",
      "Accuracy: 0.5384\n",
      "Precision: 0.6761\n",
      "Recall: 0.3395\n",
      "F1 Score: 0.4520\n",
      "Average Tokens per Article: 53.99\n",
      "Total Cost: $0.13\n",
      "Average Processing Time: 0.87s\n",
      "Evaluating with template 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 2: 100%|██████████| 1250/1250 [20:37<00:00,  1.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 2:\n",
      "Accuracy: 0.5480\n",
      "Precision: 0.7166\n",
      "Recall: 0.3210\n",
      "F1 Score: 0.4433\n",
      "Average Tokens per Article: 114.97\n",
      "Total Cost: $0.29\n",
      "Average Processing Time: 0.99s\n",
      "Evaluating with template 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Template 3: 100%|██████████| 1250/1250 [33:41<00:00,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1250 valid predictions for evaluation\n",
      "\n",
      "Results for FAKENEWS - Template 3:\n",
      "Accuracy: 0.4824\n",
      "Precision: 0.7250\n",
      "Recall: 0.1241\n",
      "F1 Score: 0.2119\n",
      "Average Tokens per Article: 113.38\n",
      "Total Cost: $0.28\n",
      "Average Processing Time: 1.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n# Cross dataset evaluation:\")\n",
    "# Two prompt templates of LIAR model being tested on FakeNewsNet data\n",
    "fake_news_data_using_liar_model = evaluate_model(fake_news_test_data, 'LIAR', batch_size=10, model=liar_model_id, temperature=0.3)\n",
    "\n",
    "# Three prompt templates of LIAR model being tested on FakeNewsNet data\n",
    "liar_data_using_fake_news_model = evaluate_model(liar_test, 'FAKENEWS', batch_size=3, model=fakenews_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = client.fine_tuning.jobs.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID: ftjob-aa2UJ5qWMWVCiwvB4l2aFl2W, Status: succeeded, Fine Tune ID: ft:gpt-3.5-turbo-0125:university-edinburgh:fakenews-balanced-e4:BHlR3dso\n",
      "Job ID: ftjob-7sTZSC93NFkGkTYQIPfHcK4C, Status: succeeded, Fine Tune ID: ft:gpt-3.5-turbo-0125:university-edinburgh:liar-balanced-e4:BHkIJsSZ\n"
     ]
    }
   ],
   "source": [
    "job_ids = [\"ftjob-7sTZSC93NFkGkTYQIPfHcK4C\", \"ftjob-aa2UJ5qWMWVCiwvB4l2aFl2W\"]  \n",
    "\n",
    "for job in jobs:\n",
    "    if job.id in job_ids:\n",
    "        print(f\"Job ID: {job.id}, Status: {job.status}, Fine Tune ID: {job.fine_tuned_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
